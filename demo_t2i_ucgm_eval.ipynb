{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a36078d9-c788-4323-b9af-88225e6c6c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import torch.backends.cuda\n",
    "import torch.distributed as dist\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "import os\n",
    "import random\n",
    "import argparse\n",
    "import json\n",
    "import time\n",
    "import itertools\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "from copy import deepcopy\n",
    "from accelerate import Accelerator\n",
    "from torchvision import transforms\n",
    "from transformers import CLIPImageProcessor, CLIPTokenizer, CLIPTextModel, CLIPVisionModelWithProjection\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel\n",
    "from PIL import Image\n",
    "\n",
    "# local imports\n",
    "from networks import NETWORKS\n",
    "from optimers import OPTIMERS\n",
    "from autoencs import AUTOENCS\n",
    "from methodes import METHODES\n",
    "from utilities import ImgLatentDataset\n",
    "from utilities import create_logger, load_config\n",
    "from utilities import set_seed, update_ema, remove_module_prefix, remove_module_all\n",
    "\n",
    "from ip_adapter.ip_adapter import ImageProjModel\n",
    "from ip_adapter.utils import is_torch2_available, get_generator\n",
    "if is_torch2_available():\n",
    "    from ip_adapter.attention_processor import IPAttnProcessor2_0 as IPAttnProcessor, AttnProcessor2_0 as AttnProcessor\n",
    "else:\n",
    "    from ip_adapter.attention_processor import IPAttnProcessor, AttnProcessor\n",
    "\n",
    "\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, json_file, tokenizer, size=512, t_drop_rate=0.05, i_drop_rate=0.05, ti_drop_rate=0.05, image_root_path=\"\", vae=None, compute_latent_stats=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.size = size\n",
    "        self.i_drop_rate = i_drop_rate\n",
    "        self.t_drop_rate = t_drop_rate\n",
    "        self.ti_drop_rate = ti_drop_rate\n",
    "        self.image_root_path = image_root_path\n",
    "        self.vae = vae\n",
    "\n",
    "        self.data = json.load(open(json_file))\n",
    "\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(self.size, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "            transforms.CenterCrop(self.size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5], [0.5]),\n",
    "        ])\n",
    "        self.clip_image_processor = CLIPImageProcessor()\n",
    "        \n",
    "        # Initialize latent statistics\n",
    "        self._latent_mean = None\n",
    "        self._latent_std = None\n",
    "        self.latent_multiplier = 1.0\n",
    "        \n",
    "        # Compute latent statistics if requested\n",
    "        if compute_latent_stats and vae is not None:\n",
    "            self._compute_latent_statistics()\n",
    "        \n",
    "    def _compute_latent_statistics(self, sample_size=128, batch_size=16):\n",
    "        \"\"\"\n",
    "        Compute mean and std of latent representations\n",
    "        \n",
    "        Args:\n",
    "            sample_size: Number of samples to use for statistics computation\n",
    "            batch_size: Batch size for processing\n",
    "        \"\"\"\n",
    "        print(\"Computing latent statistics...\")\n",
    "        \n",
    "        # Sample subset of data\n",
    "        sample_indices = np.random.choice(len(self.data), \n",
    "                                        min(sample_size, len(self.data)), \n",
    "                                        replace=False)\n",
    "        \n",
    "        latents = []\n",
    "        \n",
    "        # Temporarily disable dropout for consistent statistics\n",
    "        original_i_drop = self.i_drop_rate\n",
    "        original_t_drop = self.t_drop_rate\n",
    "        original_ti_drop = self.ti_drop_rate\n",
    "        self.i_drop_rate = 0.0\n",
    "        self.t_drop_rate = 0.0\n",
    "        self.ti_drop_rate = 0.0\n",
    "        \n",
    "        self.vae.eval()\n",
    "        with torch.no_grad():\n",
    "            for i in tqdm(range(0, len(sample_indices), batch_size), desc=\"Computing latent stats\"):\n",
    "                batch_indices = sample_indices[i:i+batch_size]\n",
    "                batch_images = []\n",
    "                \n",
    "                for idx in batch_indices:\n",
    "                    item = self.__getitem__(idx)\n",
    "                    batch_images.append(item[\"image\"])\n",
    "                \n",
    "                # Stack batch\n",
    "                batch_tensor = torch.stack(batch_images).to(self.vae.device, dtype=self.vae.dtype)\n",
    "                \n",
    "                posterior = self.vae.encode(batch_tensor).latent_dist\n",
    "                batch_latents = posterior.sample()  # or posterior.mean\n",
    "                \n",
    "                latents.append(batch_latents.cpu())\n",
    "        \n",
    "        # Restore original dropout rates\n",
    "        self.i_drop_rate = original_i_drop\n",
    "        self.t_drop_rate = original_t_drop\n",
    "        self.ti_drop_rate = original_ti_drop\n",
    "        \n",
    "        # Concatenate all latents\n",
    "        all_latents = torch.cat(latents, dim=0)\n",
    "        print(f\"Dimensions of all latents: {all_latents.shape}\")\n",
    "        \n",
    "        # Compute statistics\n",
    "        # Flatten spatial dimensions but keep channel dimension\n",
    "        # Shape: [N, C, H, W] -> [1, C, 1, 1]\n",
    "        self._latent_mean = all_latents.mean(dim=[0, 2, 3], keepdim=True)  # [1, C, 1, 1]\n",
    "        self._latent_std = all_latents.std(dim=[0, 2, 3], keepdim=True)  # [1, C, 1, 1]\n",
    "\n",
    "        # Prevent division by zero\n",
    "        self._latent_std = torch.clamp(self._latent_std, min=1e-6)\n",
    "        \n",
    "        # Set latent multiplier (can be tuned based on your needs)\n",
    "        self.latent_multiplier = 1.0 / self._latent_std.mean().item()\n",
    "        \n",
    "        print(f\"Latent mean shape: {self._latent_mean.shape}\")\n",
    "        print(f\"Latent std shape: {self._latent_std.shape}\")\n",
    "        print(f\"Latent multiplier: {self.latent_multiplier}\")\n",
    "        print(f\"Mean of latent mean: {self._latent_mean.mean().item():.6f}\")\n",
    "        print(f\"Mean of latent std: {self._latent_std.mean().item():.6f}\")\n",
    "\n",
    "    def get_latent_stats_cuda(self):\n",
    "        \"\"\"\n",
    "        Return latent statistics on CUDA\n",
    "        \"\"\"\n",
    "        if self._latent_mean is None or self._latent_std is None:\n",
    "            raise ValueError(\"Latent statistics not computed. Set compute_latent_stats=True during initialization.\")\n",
    "        \n",
    "        return (\n",
    "            self._latent_mean.cuda(),\n",
    "            self._latent_std.cuda(),\n",
    "            self.latent_multiplier\n",
    "        )\n",
    "    \n",
    "    def save_latent_stats(self, filepath):\n",
    "        \"\"\"\n",
    "        Save latent statistics to file\n",
    "        \"\"\"\n",
    "        if self._latent_mean is None or self._latent_std is None:\n",
    "            raise ValueError(\"No latent statistics to save.\")\n",
    "        \n",
    "        torch.save({\n",
    "            'latent_mean': self._latent_mean,\n",
    "            'latent_std': self._latent_std,\n",
    "            'latent_multiplier': self.latent_multiplier\n",
    "        }, filepath)\n",
    "        print(f\"Latent statistics saved to {filepath}\")\n",
    "    \n",
    "    def load_latent_stats(self, filepath):\n",
    "        \"\"\"\n",
    "        Load latent statistics from file\n",
    "        \"\"\"\n",
    "        stats = torch.load(filepath, map_location='cpu')\n",
    "        self._latent_mean = stats['latent_mean']\n",
    "        self._latent_std = stats['latent_std']\n",
    "        self.latent_multiplier = stats['latent_multiplier']\n",
    "        print(f\"Latent statistics loaded from {filepath}\")\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx] \n",
    "        text = item[\"text\"]\n",
    "        image_file = item[\"image_file\"]\n",
    "        \n",
    "        # read image\n",
    "        raw_image = Image.open(os.path.join(self.image_root_path, image_file))\n",
    "        image = self.transform(raw_image.convert(\"RGB\"))\n",
    "        clip_image = self.clip_image_processor(images=raw_image, return_tensors=\"pt\").pixel_values\n",
    "        \n",
    "        # drop\n",
    "        drop_image_embed = 0\n",
    "        rand_num = random.random()\n",
    "        if rand_num < self.i_drop_rate:\n",
    "            drop_image_embed = 1\n",
    "        elif rand_num < (self.i_drop_rate + self.t_drop_rate):\n",
    "            text = \"\"\n",
    "        elif rand_num < (self.i_drop_rate + self.t_drop_rate + self.ti_drop_rate):\n",
    "            text = \"\"\n",
    "            drop_image_embed = 1\n",
    "        # get text and tokenize\n",
    "        text_input_ids = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.tokenizer.model_max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ).input_ids\n",
    "        \n",
    "        return {\n",
    "            \"image\": image,\n",
    "            \"text_input_ids\": text_input_ids,\n",
    "            \"clip_image\": clip_image,\n",
    "            \"drop_image_embed\": drop_image_embed\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "\n",
    "def collate_fn(data):\n",
    "    images = torch.stack([example[\"image\"] for example in data])\n",
    "    text_input_ids = torch.cat([example[\"text_input_ids\"] for example in data], dim=0)\n",
    "    clip_images = torch.cat([example[\"clip_image\"] for example in data], dim=0)\n",
    "    drop_image_embeds = [example[\"drop_image_embed\"] for example in data]\n",
    "\n",
    "    return {\n",
    "        \"images\": images,\n",
    "        \"text_input_ids\": text_input_ids,\n",
    "        \"clip_images\": clip_images,\n",
    "        \"drop_image_embeds\": drop_image_embeds\n",
    "    }\n",
    "    \n",
    "\n",
    "class IPAdapter(torch.nn.Module):\n",
    "    \"\"\"IP-Adapter\"\"\"\n",
    "    def __init__(self, unet, image_proj_model, adapter_modules, ckpt_path=None):\n",
    "        super().__init__()\n",
    "        self.unet = unet\n",
    "        self.image_proj_model = image_proj_model\n",
    "        self.adapter_modules = adapter_modules\n",
    "\n",
    "        if ckpt_path is not None:\n",
    "            self.load_from_checkpoint(ckpt_path)\n",
    "    \n",
    "    def get_encoder_hidden_states(self, text_embeds, image_embeds):\n",
    "        \"\"\"\n",
    "        Get encoder hidden states with image embeddings projected by image_proj_model.\n",
    "        \"\"\"\n",
    "        ip_tokens = self.image_proj_model(image_embeds)\n",
    "        encoder_hidden_states = torch.cat([text_embeds, ip_tokens], dim=1)\n",
    "        return encoder_hidden_states\n",
    "    \n",
    "    def forward(self, noisy_latents, timesteps, encoder_hidden_states):\n",
    "        \"\"\"\n",
    "        Forward pass for the IP-Adapter.\n",
    "        Args:\n",
    "            noisy_latents: Noisy latents input to the UNet.\n",
    "            timesteps: Timesteps for the diffusion process.\n",
    "            encoder_hidden_states: Text embeddings concatenated with image embeddings projected by image_proj_model.\n",
    "        Returns:\n",
    "            noise_pred: Predicted noise residual from the UNet.\n",
    "        \"\"\"\n",
    "        # Predict the noise residual\n",
    "        noise_pred = self.unet(noisy_latents, timesteps, encoder_hidden_states).sample\n",
    "        return noise_pred\n",
    "\n",
    "    def load_from_checkpoint(self, ckpt_path: str):\n",
    "        # Calculate original checksums\n",
    "        orig_ip_proj_sum = torch.sum(torch.stack([torch.sum(p) for p in self.image_proj_model.parameters()]))\n",
    "        orig_adapter_sum = torch.sum(torch.stack([torch.sum(p) for p in self.adapter_modules.parameters()]))\n",
    "\n",
    "        state_dict = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "\n",
    "        # Load state dict for image_proj_model and adapter_modules\n",
    "        self.image_proj_model.load_state_dict(state_dict[\"image_proj\"], strict=True)\n",
    "        self.adapter_modules.load_state_dict(state_dict[\"ip_adapter\"], strict=True)\n",
    "\n",
    "        # Calculate new checksums\n",
    "        new_ip_proj_sum = torch.sum(torch.stack([torch.sum(p) for p in self.image_proj_model.parameters()]))\n",
    "        new_adapter_sum = torch.sum(torch.stack([torch.sum(p) for p in self.adapter_modules.parameters()]))\n",
    "\n",
    "        # Verify if the weights have changed\n",
    "        assert orig_ip_proj_sum != new_ip_proj_sum, \"Weights of image_proj_model did not change!\"\n",
    "        assert orig_adapter_sum != new_adapter_sum, \"Weights of adapter_modules did not change!\"\n",
    "\n",
    "        print(f\"Successfully loaded weights from checkpoint {ckpt_path}\")\n",
    "\n",
    "\n",
    "def get_model(train_config, accelerator):\n",
    "    pretrained_model_name_or_path = train_config['model'][\"pretrained_model_name_or_path\"]\n",
    "    image_encoder_path = train_config['model'][\"image_encoder_path\"]\n",
    "    tokenizer = CLIPTokenizer.from_pretrained(pretrained_model_name_or_path, subfolder=\"tokenizer\")\n",
    "    text_encoder = CLIPTextModel.from_pretrained(pretrained_model_name_or_path, subfolder=\"text_encoder\")\n",
    "    vae = AutoencoderKL.from_pretrained(pretrained_model_name_or_path, subfolder=\"vae\")\n",
    "    unet = UNet2DConditionModel.from_pretrained(pretrained_model_name_or_path, subfolder=\"unet\")\n",
    "    image_encoder = CLIPVisionModelWithProjection.from_pretrained(image_encoder_path)\n",
    "    # freeze parameters of models to save more memory\n",
    "    # unet.requires_grad_(False)\n",
    "    vae.requires_grad_(False)\n",
    "    text_encoder.requires_grad_(False)\n",
    "    image_encoder.requires_grad_(False)\n",
    "\n",
    "    # ip-adapter\n",
    "    image_proj_model = ImageProjModel(\n",
    "        cross_attention_dim=unet.config.cross_attention_dim,\n",
    "        clip_embeddings_dim=image_encoder.config.projection_dim,\n",
    "        clip_extra_context_tokens=4,\n",
    "    )\n",
    "    # init adapter modules\n",
    "    attn_procs = {}\n",
    "    unet_sd = unet.state_dict()\n",
    "    for name in unet.attn_processors.keys():\n",
    "        cross_attention_dim = None if name.endswith(\"attn1.processor\") else unet.config.cross_attention_dim\n",
    "        if name.startswith(\"mid_block\"):\n",
    "            hidden_size = unet.config.block_out_channels[-1]\n",
    "        elif name.startswith(\"up_blocks\"):\n",
    "            block_id = int(name[len(\"up_blocks.\")])\n",
    "            hidden_size = list(reversed(unet.config.block_out_channels))[block_id]\n",
    "        elif name.startswith(\"down_blocks\"):\n",
    "            block_id = int(name[len(\"down_blocks.\")])\n",
    "            hidden_size = unet.config.block_out_channels[block_id]\n",
    "        if cross_attention_dim is None:\n",
    "            attn_procs[name] = AttnProcessor()\n",
    "        else:\n",
    "            layer_name = name.split(\".processor\")[0]\n",
    "            weights = {\n",
    "                \"to_k_ip.weight\": unet_sd[layer_name + \".to_k.weight\"],\n",
    "                \"to_v_ip.weight\": unet_sd[layer_name + \".to_v.weight\"],\n",
    "            }\n",
    "            attn_procs[name] = IPAttnProcessor(hidden_size=hidden_size, cross_attention_dim=cross_attention_dim)\n",
    "            attn_procs[name].load_state_dict(weights)\n",
    "    unet.set_attn_processor(attn_procs)\n",
    "    adapter_modules = torch.nn.ModuleList(unet.attn_processors.values())\n",
    "    \n",
    "    ip_adapter = IPAdapter(unet, image_proj_model, adapter_modules, train_config['model'][\"pretrained_ip_adapter_path\"])\n",
    "    ip_adapter.requires_grad_(False)\n",
    "    \n",
    "    weight_dtype = torch.float16\n",
    "    # if accelerator.mixed_precision == \"fp16\":\n",
    "    #     weight_dtype = torch.float16\n",
    "    # elif accelerator.mixed_precision == \"bf16\":\n",
    "    #     weight_dtype = torch.bfloat16\n",
    "    # unet.to(accelerator.device, dtype=weight_dtype)\n",
    "    vae.to(accelerator.device, dtype=weight_dtype)\n",
    "    text_encoder.to(accelerator.device, dtype=weight_dtype)\n",
    "    image_encoder.to(accelerator.device, dtype=weight_dtype)\n",
    "    # add\n",
    "    unet.to(accelerator.device, dtype=weight_dtype)\n",
    "    image_proj_model.to(accelerator.device, dtype=weight_dtype)\n",
    "    ip_adapter.to(accelerator.device, dtype=weight_dtype)\n",
    "\n",
    "\n",
    "    # params_to_opt = itertools.chain(ip_adapter.image_proj_model.parameters(),  ip_adapter.adapter_modules.parameters())\n",
    "    params_to_opt = itertools.chain(unet.parameters())\n",
    "\n",
    "    return ip_adapter, params_to_opt, tokenizer, vae, text_encoder, image_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2a71bc9-de68-4de4-b6c3-16c92fac3e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: Experiment directory created at ./outputs/tuning_few_steps/in1k256_sd15_klae_linear\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using config: ./configs/tuning_few_steps/in1k256_sd15_klae_linear.yaml\n",
      "{'output_dir': './outputs', 'data': {'json_file': './sd_data_2k/captions.json', 'image_size': 512, 'image_root_path': './sd_data_2k', 'num_workers': 8}, 'vae': {'type': 'klae_ema_f8c4', 'downsample_ratio': 8}, 'model': {'pretrained_model_name_or_path': '/data/pretrained_models/stable-diffusion-v1-5', 'image_encoder_path': '/data/pretrained_models/IP-Adapter/models/image_encoder', 'pretrained_ip_adapter_path': '/data/pretrained_models/IP-Adapter/models/ip-adapter_sd15.bin'}, 'train': {'no_reopt': True, 'no_reuni': True, 'no_buffer': False, 'max_steps': 1, 'global_batch_size': 32, 'global_seed': 0, 'log_every': 50, 'ckpt_every': 1, 'ema_decay': 0.99}, 'optimizer': {'type': 'RAdam', 'lr': 0.0001, 'weight_decay': 0.0, 'beta1': 0.9, 'beta2': 0.999, 'max_grad_norm': 0.1}, 'transport': {'type': 'DDIM', 'lab_drop_ratio': 0.1, 'consistc_ratio': 0.0, 'enhanced_ratio': 0.0, 'enhanced_style': None, 'scaled_cbl_eps': 0.0, 'ema_decay_rate': 1.0, 'enhanced_range': [0.0, 0.75], 'time_dist_ctrl': [1.0, 1.0, 1.0], 'weight_funcion': None, 'wt_cosine_loss': False}, 'sample': {'ckpt': '0003200.pt', 'type': 'UNI', 'cfg_scale': 0.0, 'cfg_interval': [0.0, 0.7], 'sampling_steps': 2, 'stochast_ratio': 1.0, 'extrapol_ratio': 0.0, 'sampling_order': 1, 'time_dist_ctrl': [1.0, 1.0, 1.0], 'rfba_gap_steps': [0.001, 0.6], 'per_batch_size': 50, 'fid_sample_num': 50000}, 'exp_name': 'tuning_few_steps/in1k256_sd15_klae_linear'}\n",
      "Successfully loaded weights from checkpoint /data/pretrained_models/IP-Adapter/models/ip-adapter_sd15.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: SD15 parameters: 878.69M\n",
      "\u001b[32mINFO\u001b[0m: Optimizer: RAdam, lr=0.0001, beta1=0.9, beta2=0.999\n",
      "\u001b[32mINFO\u001b[0m: Use cosine loss: False\n",
      "\u001b[32mINFO\u001b[0m: Use weight func: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] 718, params to optimize\n",
      "Computing latent statistics...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing latent stats: 100%|██████████| 8/8 [00:04<00:00,  1.71it/s]\n",
      "\u001b[32mINFO\u001b[0m: Batch size 32 per gpu, with 32 global batch size\n",
      "\u001b[32mINFO\u001b[0m: Starting training from scratch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of all latents: torch.Size([128, 4, 64, 64])\n",
      "Latent mean shape: torch.Size([1, 4, 1, 1])\n",
      "Latent std shape: torch.Size([1, 4, 1, 1])\n",
      "Latent multiplier: 0.22107081174438686\n",
      "Mean of latent mean: 0.339600\n",
      "Mean of latent std: 4.523438\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 183\u001b[39m\n\u001b[32m    180\u001b[39m loss = unigen.training_step(model, x, y)\n\u001b[32m    182\u001b[39m opt.zero_grad()\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m \u001b[43maccelerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mmax_grad_norm\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m train_config[\u001b[33m\"\u001b[39m\u001b[33moptimizer\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    185\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m accelerator.sync_gradients:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ucgm/lib/python3.11/site-packages/accelerate/accelerator.py:1989\u001b[39m, in \u001b[36mAccelerator.backward\u001b[39m\u001b[34m(self, loss, **kwargs)\u001b[39m\n\u001b[32m   1987\u001b[39m     \u001b[38;5;28mself\u001b[39m.scaler.scale(loss).backward(**kwargs)\n\u001b[32m   1988\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1989\u001b[39m     \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ucgm/lib/python3.11/site-packages/torch/_tensor.py:581\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    571\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    572\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    573\u001b[39m         Tensor.backward,\n\u001b[32m    574\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    579\u001b[39m         inputs=inputs,\n\u001b[32m    580\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m581\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    582\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    583\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ucgm/lib/python3.11/site-packages/torch/autograd/__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ucgm/lib/python3.11/site-packages/torch/autograd/graph.py:825\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    823\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    824\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m825\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    826\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    827\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    829\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mRuntimeError\u001b[39m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# read config\n",
    "import os\n",
    "os.environ[\"NCCL_P2P_DISABLE\"] = \"1\"\n",
    "os.environ[\"NCCL_IB_DISABLE\"] = \"1\"\n",
    "\n",
    "args_config = \"./configs/tuning_few_steps/in1k256_sd15_klae_linear.yaml\"\n",
    "accelerator = Accelerator()\n",
    "train_config = load_config(args_config)\n",
    "par_path = args_config.split(\"/\")\n",
    "train_config[\"exp_name\"] = os.path.join(par_path[-2], par_path[-1].split(\".\")[0])\n",
    "\n",
    "print(f\"Using config: {args_config}\")\n",
    "print(train_config)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Training a generative model.\n",
    "\"\"\"\n",
    "# Setup accelerator:\n",
    "device = accelerator.device\n",
    "rank = accelerator.process_index\n",
    "seed = train_config[\"train\"][\"global_seed\"] * accelerator.num_processes + rank\n",
    "set_seed(seed)\n",
    "\n",
    "experiment_dir = f\"{train_config['output_dir']}/{train_config['exp_name']}\"\n",
    "checkpoint_dir = f\"{experiment_dir}/checkpoints\"\n",
    "\n",
    "# Setup an experiment folder:\n",
    "if accelerator.is_main_process:\n",
    "    os.makedirs(train_config[\"output_dir\"], exist_ok=True)\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    logger = create_logger(experiment_dir, \"train\")\n",
    "    logger.info(f\"Experiment directory created at {experiment_dir}\")\n",
    "\n",
    "# Create model:\n",
    "downsample_ratio = train_config[\"vae\"][\"downsample_ratio\"]\n",
    "assert (\n",
    "    train_config[\"data\"][\"image_size\"] % downsample_ratio == 0\n",
    "), \"Image size must be divisible by 8 (for the VAE encoder).\"\n",
    "latent_size = train_config[\"data\"][\"image_size\"] // downsample_ratio\n",
    "\n",
    "# Load scheduler, tokenizer and models.\n",
    "model, params_to_opt, tokenizer, vae, text_encoder, image_encoder = get_model(train_config, accelerator)\n",
    "model = model.to(device)\n",
    "params_to_opt_list = list(params_to_opt)\n",
    "print(f\"[{rank}] {len(params_to_opt_list)}, params to optimize\")\n",
    "\n",
    "ema = deepcopy(model).requires_grad_(False).to(device)\n",
    "\n",
    "if accelerator.is_main_process:\n",
    "    demoimages_dir = f\"{experiment_dir}/demoimages\"\n",
    "    os.makedirs(demoimages_dir, exist_ok=True)\n",
    "\n",
    "unigen = METHODES[\"unigen\"](\n",
    "    transport_type=train_config[\"transport\"][\"type\"],\n",
    "    lab_drop_ratio=train_config[\"transport\"][\"lab_drop_ratio\"],\n",
    "    consistc_ratio=train_config[\"transport\"][\"consistc_ratio\"],\n",
    "    enhanced_ratio=train_config[\"transport\"][\"enhanced_ratio\"],\n",
    "    enhanced_style=train_config[\"transport\"][\"enhanced_style\"],\n",
    "    scaled_cbl_eps=train_config[\"transport\"][\"scaled_cbl_eps\"],\n",
    "    ema_decay_rate=train_config[\"transport\"][\"ema_decay_rate\"],\n",
    "    enhanced_range=train_config[\"transport\"][\"enhanced_range\"],\n",
    "    time_dist_ctrl=train_config[\"transport\"][\"time_dist_ctrl\"],\n",
    "    wt_cosine_loss=train_config[\"transport\"][\"wt_cosine_loss\"],\n",
    "    weight_funcion=train_config[\"transport\"][\"weight_funcion\"],\n",
    ")\n",
    "if accelerator.is_main_process:\n",
    "    logger.info(\n",
    "        f\"SD15 parameters: {sum(p.numel() for p in params_to_opt_list) / 1e6:.2f}M\"\n",
    "    )\n",
    "    logger.info(\n",
    "        f\"Optimizer: {train_config['optimizer']['type']}, lr={train_config['optimizer']['lr']}, beta1={train_config['optimizer']['beta1']}, beta2={train_config['optimizer']['beta2']}\"\n",
    "    )\n",
    "    logger.info(f'Use cosine loss: {train_config[\"transport\"][\"wt_cosine_loss\"]}')\n",
    "    logger.info(f'Use weight func: {train_config[\"transport\"][\"weight_funcion\"]}')\n",
    "\n",
    "opt = OPTIMERS[train_config[\"optimizer\"][\"type\"]](\n",
    "    params_to_opt_list,\n",
    "    lr=train_config[\"optimizer\"][\"lr\"],\n",
    "    weight_decay=train_config[\"optimizer\"][\"weight_decay\"],\n",
    "    betas=(train_config[\"optimizer\"][\"beta1\"], train_config[\"optimizer\"][\"beta2\"]),\n",
    ")\n",
    "\n",
    "# tokenizer = CLIPTokenizer.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"tokenizer\")\n",
    "dataset = MyDataset(\n",
    "    json_file=train_config[\"data\"][\"json_file\"],\n",
    "    tokenizer=tokenizer,\n",
    "    size=train_config[\"data\"][\"image_size\"],\n",
    "    image_root_path=train_config[\"data\"][\"image_root_path\"],\n",
    "    vae=vae,\n",
    "    compute_latent_stats=True,\n",
    ")\n",
    "\n",
    "batch_size_per_gpu = (\n",
    "    train_config[\"train\"][\"global_batch_size\"] // accelerator.num_processes\n",
    ")\n",
    "\n",
    "global_batch_size = batch_size_per_gpu * accelerator.num_processes\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size_per_gpu,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=train_config[\"data\"][\"num_workers\"],\n",
    "    pin_memory=True,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "if accelerator.is_main_process:\n",
    "    mean, stad, latent_multiplier = (\n",
    "        dataset._latent_mean.cuda(),\n",
    "        dataset._latent_std.cuda(),\n",
    "        dataset.latent_multiplier,\n",
    "    )\n",
    "    # logger.info(\n",
    "    #     f\"Dataset contains {len(dataset):,} images {train_config['data']['data_path']}\"\n",
    "    # )\n",
    "    logger.info(\n",
    "        f\"Batch size {batch_size_per_gpu} per gpu, with {global_batch_size} global batch size\"\n",
    "    )\n",
    "\n",
    "if \"ckpt\" in train_config[\"train\"]:\n",
    "    checkpoint_path = f\"{checkpoint_dir}/{train_config['train']['ckpt']}\"\n",
    "    checkpoint = torch.load(\n",
    "        checkpoint_path, map_location=lambda storage, loc: storage\n",
    "    )\n",
    "    model.load_state_dict(checkpoint[\"model\"])\n",
    "    if train_config[\"train\"][\"no_reopt\"] is not True:\n",
    "        opt.load_state_dict(checkpoint[\"opt\"])\n",
    "    if train_config[\"train\"][\"no_reuni\"] is not True:\n",
    "        if ((unigen.cor > 0.0) or (unigen.enr > 0.0)) and unigen.emd > 0.0:\n",
    "            unigen.mod = deepcopy(model).requires_grad_(False).to(device)\n",
    "        unigen.load_state_dict(checkpoint[\"unigen\"])\n",
    "    ema.load_state_dict(checkpoint[\"ema\"])\n",
    "    train_steps = int(checkpoint_path.split(\"/\")[-1].split(\".\")[0])\n",
    "    del checkpoint\n",
    "    if accelerator.is_main_process:\n",
    "        logger.info(f\"Loaded checkpoint at: {checkpoint_path}.\")\n",
    "else:\n",
    "    train_steps = 0\n",
    "    update_ema(ema, model, decay=0)\n",
    "    if accelerator.is_main_process:\n",
    "        logger.info(\"Starting training from scratch.\")\n",
    "\n",
    "# Prepare models for training:\n",
    "model.train()\n",
    "ema.eval()\n",
    "if train_config[\"train\"][\"no_buffer\"] is True:\n",
    "    model = DDP(model, device_ids=[rank], broadcast_buffers=False)\n",
    "model, opt, loader, unigen = accelerator.prepare(model, opt, loader, unigen)\n",
    "\n",
    "# Variables for monitoring/logging purposes:\n",
    "log_steps = 0\n",
    "running_loss = 0\n",
    "start_time = time()\n",
    "\n",
    "while True:\n",
    "    for batch in loader:\n",
    "        # Convert images to latent space\n",
    "        with torch.no_grad():\n",
    "            latents = vae.encode(batch[\"images\"].to(accelerator.device, dtype=vae.dtype)).latent_dist.sample()\n",
    "            latents = latents * vae.config.scaling_factor\n",
    "\n",
    "            image_embeds = image_encoder(batch[\"clip_images\"].to(accelerator.device, dtype=image_encoder.dtype)).image_embeds\n",
    "            text_embeds = text_encoder(batch[\"text_input_ids\"].to(accelerator.device))[0]\n",
    "\n",
    "        image_embeds_ = []\n",
    "        for image_embed, drop_image_embed in zip(image_embeds, batch[\"drop_image_embeds\"]):\n",
    "            if drop_image_embed == 1:\n",
    "                image_embeds_.append(torch.zeros_like(image_embed))\n",
    "            else:\n",
    "                image_embeds_.append(image_embed)\n",
    "        image_embeds = torch.stack(image_embeds_)\n",
    "\n",
    "        x = latents\n",
    "        y = model.get_encoder_hidden_states(text_embeds, image_embeds)\n",
    "\n",
    "        loss = unigen.training_step(model, x, y)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        accelerator.backward(loss)\n",
    "        if \"max_grad_norm\" in train_config[\"optimizer\"]:\n",
    "            if accelerator.sync_gradients:\n",
    "                accelerator.clip_grad_norm_(\n",
    "                    model.parameters(), train_config[\"optimizer\"][\"max_grad_norm\"]\n",
    "                )\n",
    "        for param in model.parameters():\n",
    "            if param.grad is not None:\n",
    "                torch.nan_to_num_(param.grad, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        opt.step()\n",
    "        update_ema(ema, model, train_config[\"train\"][\"ema_decay\"])\n",
    "\n",
    "        # Log loss values:\n",
    "        running_loss += loss.item()\n",
    "        log_steps += 1\n",
    "        train_steps += 1\n",
    "        if train_steps % train_config[\"train\"][\"log_every\"] == 0:\n",
    "            # Measure training speed:\n",
    "            torch.cuda.synchronize()\n",
    "            end_time = time()\n",
    "            steps_per_sec = log_steps / (end_time - start_time)\n",
    "            # Reduce loss history over all processes:\n",
    "            avg_loss = torch.tensor(running_loss / log_steps, device=device)\n",
    "            dist.all_reduce(avg_loss, op=dist.ReduceOp.SUM)\n",
    "            avg_loss = avg_loss.item() / dist.get_world_size()\n",
    "            if accelerator.is_main_process:\n",
    "                logger.info(\n",
    "                    f\"(step={train_steps:07d}) Train Loss: {avg_loss:.4f}, Train Steps/Sec: {steps_per_sec:.2f}\"\n",
    "                )\n",
    "            # Reset monitoring variables:\n",
    "            running_loss = 0\n",
    "            log_steps = 0\n",
    "            start_time = time()\n",
    "\n",
    "        # Save checkpoint:\n",
    "        if (\n",
    "            train_steps % train_config[\"train\"][\"ckpt_every\"] == 0\n",
    "            and train_steps > 0\n",
    "        ):\n",
    "            if accelerator.is_main_process:\n",
    "                checkpoint = {\n",
    "                    \"model\": remove_module_prefix(model.state_dict()),\n",
    "                    \"ema\": ema.state_dict(),\n",
    "                    \"opt\": opt.state_dict(),\n",
    "                    \"unigen\": remove_module_all(unigen.state_dict()),\n",
    "                    \"config\": train_config,\n",
    "                }\n",
    "                checkpoint_path = f\"{checkpoint_dir}/{train_steps:07d}.pt\"\n",
    "                torch.save(checkpoint, checkpoint_path)\n",
    "                logger.info(f\"Saved checkpoint to {checkpoint_path}\")\n",
    "\n",
    "            # dist.barrier()\n",
    "\n",
    "        if train_steps >= train_config[\"train\"][\"max_steps\"]:\n",
    "            break\n",
    "    if train_steps >= train_config[\"train\"][\"max_steps\"]:\n",
    "        break\n",
    "if accelerator.is_main_process:\n",
    "    logger.info(\"Done!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc61e24d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IPAdapter(\n",
       "  (unet): UNet2DConditionModel(\n",
       "    (conv_in): Conv2d(4, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (time_proj): Timesteps()\n",
       "    (time_embedding): TimestepEmbedding(\n",
       "      (linear_1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "      (act): SiLU()\n",
       "      (linear_2): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "    )\n",
       "    (down_blocks): ModuleList(\n",
       "      (0): CrossAttnDownBlock2D(\n",
       "        (attentions): ModuleList(\n",
       "          (0-1): 2 x Transformer2DModel(\n",
       "            (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
       "            (proj_in): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (transformer_blocks): ModuleList(\n",
       "              (0): BasicTransformerBlock(\n",
       "                (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn1): Attention(\n",
       "                  (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "                  (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
       "                  (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
       "                  (to_out): ModuleList(\n",
       "                    (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                  (processor): AttnProcessor2_0()\n",
       "                )\n",
       "                (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn2): Attention(\n",
       "                  (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "                  (to_k): Linear(in_features=768, out_features=320, bias=False)\n",
       "                  (to_v): Linear(in_features=768, out_features=320, bias=False)\n",
       "                  (to_out): ModuleList(\n",
       "                    (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                  (processor): IPAttnProcessor2_0(\n",
       "                    (to_k_ip): Linear(in_features=768, out_features=320, bias=False)\n",
       "                    (to_v_ip): Linear(in_features=768, out_features=320, bias=False)\n",
       "                  )\n",
       "                )\n",
       "                (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "                (ff): FeedForward(\n",
       "                  (net): ModuleList(\n",
       "                    (0): GEGLU(\n",
       "                      (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
       "                    )\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                    (2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (proj_out): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (resnets): ModuleList(\n",
       "          (0-1): 2 x ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "            (conv1): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)\n",
       "            (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (downsamplers): ModuleList(\n",
       "          (0): Downsample2D(\n",
       "            (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): CrossAttnDownBlock2D(\n",
       "        (attentions): ModuleList(\n",
       "          (0-1): 2 x Transformer2DModel(\n",
       "            (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
       "            (proj_in): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (transformer_blocks): ModuleList(\n",
       "              (0): BasicTransformerBlock(\n",
       "                (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn1): Attention(\n",
       "                  (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "                  (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
       "                  (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
       "                  (to_out): ModuleList(\n",
       "                    (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                  (processor): AttnProcessor2_0()\n",
       "                )\n",
       "                (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn2): Attention(\n",
       "                  (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "                  (to_k): Linear(in_features=768, out_features=640, bias=False)\n",
       "                  (to_v): Linear(in_features=768, out_features=640, bias=False)\n",
       "                  (to_out): ModuleList(\n",
       "                    (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                  (processor): IPAttnProcessor2_0(\n",
       "                    (to_k_ip): Linear(in_features=768, out_features=640, bias=False)\n",
       "                    (to_v_ip): Linear(in_features=768, out_features=640, bias=False)\n",
       "                  )\n",
       "                )\n",
       "                (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "                (ff): FeedForward(\n",
       "                  (net): ModuleList(\n",
       "                    (0): GEGLU(\n",
       "                      (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
       "                    )\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                    (2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (proj_out): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (resnets): ModuleList(\n",
       "          (0): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "            (conv1): Conv2d(320, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
       "            (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "            (conv_shortcut): Conv2d(320, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "            (conv1): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
       "            (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (downsamplers): ModuleList(\n",
       "          (0): Downsample2D(\n",
       "            (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): CrossAttnDownBlock2D(\n",
       "        (attentions): ModuleList(\n",
       "          (0-1): 2 x Transformer2DModel(\n",
       "            (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "            (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (transformer_blocks): ModuleList(\n",
       "              (0): BasicTransformerBlock(\n",
       "                (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn1): Attention(\n",
       "                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                  (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                  (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                  (to_out): ModuleList(\n",
       "                    (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                  (processor): AttnProcessor2_0()\n",
       "                )\n",
       "                (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn2): Attention(\n",
       "                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                  (to_k): Linear(in_features=768, out_features=1280, bias=False)\n",
       "                  (to_v): Linear(in_features=768, out_features=1280, bias=False)\n",
       "                  (to_out): ModuleList(\n",
       "                    (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                  (processor): IPAttnProcessor2_0(\n",
       "                    (to_k_ip): Linear(in_features=768, out_features=1280, bias=False)\n",
       "                    (to_v_ip): Linear(in_features=768, out_features=1280, bias=False)\n",
       "                  )\n",
       "                )\n",
       "                (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "                (ff): FeedForward(\n",
       "                  (net): ModuleList(\n",
       "                    (0): GEGLU(\n",
       "                      (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "                    )\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                    (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (resnets): ModuleList(\n",
       "          (0): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "            (conv1): Conv2d(640, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "            (conv_shortcut): Conv2d(640, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "            (conv1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (downsamplers): ModuleList(\n",
       "          (0): Downsample2D(\n",
       "            (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): DownBlock2D(\n",
       "        (resnets): ModuleList(\n",
       "          (0-1): 2 x ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "            (conv1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (up_blocks): ModuleList(\n",
       "      (0): UpBlock2D(\n",
       "        (resnets): ModuleList(\n",
       "          (0-2): 3 x ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 2560, eps=1e-05, affine=True)\n",
       "            (conv1): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "            (conv_shortcut): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (upsamplers): ModuleList(\n",
       "          (0): Upsample2D(\n",
       "            (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): CrossAttnUpBlock2D(\n",
       "        (attentions): ModuleList(\n",
       "          (0-2): 3 x Transformer2DModel(\n",
       "            (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "            (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (transformer_blocks): ModuleList(\n",
       "              (0): BasicTransformerBlock(\n",
       "                (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn1): Attention(\n",
       "                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                  (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                  (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                  (to_out): ModuleList(\n",
       "                    (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                  (processor): AttnProcessor2_0()\n",
       "                )\n",
       "                (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn2): Attention(\n",
       "                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                  (to_k): Linear(in_features=768, out_features=1280, bias=False)\n",
       "                  (to_v): Linear(in_features=768, out_features=1280, bias=False)\n",
       "                  (to_out): ModuleList(\n",
       "                    (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                  (processor): IPAttnProcessor2_0(\n",
       "                    (to_k_ip): Linear(in_features=768, out_features=1280, bias=False)\n",
       "                    (to_v_ip): Linear(in_features=768, out_features=1280, bias=False)\n",
       "                  )\n",
       "                )\n",
       "                (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "                (ff): FeedForward(\n",
       "                  (net): ModuleList(\n",
       "                    (0): GEGLU(\n",
       "                      (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "                    )\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                    (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (resnets): ModuleList(\n",
       "          (0-1): 2 x ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 2560, eps=1e-05, affine=True)\n",
       "            (conv1): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "            (conv_shortcut): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (2): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 1920, eps=1e-05, affine=True)\n",
       "            (conv1): Conv2d(1920, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "            (conv_shortcut): Conv2d(1920, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (upsamplers): ModuleList(\n",
       "          (0): Upsample2D(\n",
       "            (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): CrossAttnUpBlock2D(\n",
       "        (attentions): ModuleList(\n",
       "          (0-2): 3 x Transformer2DModel(\n",
       "            (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
       "            (proj_in): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (transformer_blocks): ModuleList(\n",
       "              (0): BasicTransformerBlock(\n",
       "                (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn1): Attention(\n",
       "                  (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "                  (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
       "                  (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
       "                  (to_out): ModuleList(\n",
       "                    (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                  (processor): AttnProcessor2_0()\n",
       "                )\n",
       "                (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn2): Attention(\n",
       "                  (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "                  (to_k): Linear(in_features=768, out_features=640, bias=False)\n",
       "                  (to_v): Linear(in_features=768, out_features=640, bias=False)\n",
       "                  (to_out): ModuleList(\n",
       "                    (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                  (processor): IPAttnProcessor2_0(\n",
       "                    (to_k_ip): Linear(in_features=768, out_features=640, bias=False)\n",
       "                    (to_v_ip): Linear(in_features=768, out_features=640, bias=False)\n",
       "                  )\n",
       "                )\n",
       "                (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "                (ff): FeedForward(\n",
       "                  (net): ModuleList(\n",
       "                    (0): GEGLU(\n",
       "                      (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
       "                    )\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                    (2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (proj_out): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (resnets): ModuleList(\n",
       "          (0): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 1920, eps=1e-05, affine=True)\n",
       "            (conv1): Conv2d(1920, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
       "            (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "            (conv_shortcut): Conv2d(1920, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "            (conv1): Conv2d(1280, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
       "            (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "            (conv_shortcut): Conv2d(1280, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (2): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 960, eps=1e-05, affine=True)\n",
       "            (conv1): Conv2d(960, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
       "            (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "            (conv_shortcut): Conv2d(960, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (upsamplers): ModuleList(\n",
       "          (0): Upsample2D(\n",
       "            (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): CrossAttnUpBlock2D(\n",
       "        (attentions): ModuleList(\n",
       "          (0-2): 3 x Transformer2DModel(\n",
       "            (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
       "            (proj_in): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (transformer_blocks): ModuleList(\n",
       "              (0): BasicTransformerBlock(\n",
       "                (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn1): Attention(\n",
       "                  (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "                  (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
       "                  (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
       "                  (to_out): ModuleList(\n",
       "                    (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                  (processor): AttnProcessor2_0()\n",
       "                )\n",
       "                (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn2): Attention(\n",
       "                  (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "                  (to_k): Linear(in_features=768, out_features=320, bias=False)\n",
       "                  (to_v): Linear(in_features=768, out_features=320, bias=False)\n",
       "                  (to_out): ModuleList(\n",
       "                    (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                  (processor): IPAttnProcessor2_0(\n",
       "                    (to_k_ip): Linear(in_features=768, out_features=320, bias=False)\n",
       "                    (to_v_ip): Linear(in_features=768, out_features=320, bias=False)\n",
       "                  )\n",
       "                )\n",
       "                (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "                (ff): FeedForward(\n",
       "                  (net): ModuleList(\n",
       "                    (0): GEGLU(\n",
       "                      (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
       "                    )\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                    (2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (proj_out): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (resnets): ModuleList(\n",
       "          (0): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 960, eps=1e-05, affine=True)\n",
       "            (conv1): Conv2d(960, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)\n",
       "            (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "            (conv_shortcut): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1-2): 2 x ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "            (conv1): Conv2d(640, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)\n",
       "            (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "            (conv_shortcut): Conv2d(640, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (mid_block): UNetMidBlock2DCrossAttn(\n",
       "      (attentions): ModuleList(\n",
       "        (0): Transformer2DModel(\n",
       "          (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "          (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (transformer_blocks): ModuleList(\n",
       "            (0): BasicTransformerBlock(\n",
       "              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn1): Attention(\n",
       "                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (processor): AttnProcessor2_0()\n",
       "              )\n",
       "              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn2): Attention(\n",
       "                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_k): Linear(in_features=768, out_features=1280, bias=False)\n",
       "                (to_v): Linear(in_features=768, out_features=1280, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (processor): IPAttnProcessor2_0(\n",
       "                  (to_k_ip): Linear(in_features=768, out_features=1280, bias=False)\n",
       "                  (to_v_ip): Linear(in_features=768, out_features=1280, bias=False)\n",
       "                )\n",
       "              )\n",
       "              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (ff): FeedForward(\n",
       "                (net): ModuleList(\n",
       "                  (0): GEGLU(\n",
       "                    (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "                  )\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                  (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (resnets): ModuleList(\n",
       "        (0-1): 2 x ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (conv_norm_out): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "    (conv_act): SiLU()\n",
       "    (conv_out): Conv2d(320, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (image_proj_model): ImageProjModel(\n",
       "    (proj): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (adapter_modules): ModuleList(\n",
       "    (0): AttnProcessor2_0()\n",
       "    (1): IPAttnProcessor2_0(\n",
       "      (to_k_ip): Linear(in_features=768, out_features=320, bias=False)\n",
       "      (to_v_ip): Linear(in_features=768, out_features=320, bias=False)\n",
       "    )\n",
       "    (2): AttnProcessor2_0()\n",
       "    (3): IPAttnProcessor2_0(\n",
       "      (to_k_ip): Linear(in_features=768, out_features=320, bias=False)\n",
       "      (to_v_ip): Linear(in_features=768, out_features=320, bias=False)\n",
       "    )\n",
       "    (4): AttnProcessor2_0()\n",
       "    (5): IPAttnProcessor2_0(\n",
       "      (to_k_ip): Linear(in_features=768, out_features=640, bias=False)\n",
       "      (to_v_ip): Linear(in_features=768, out_features=640, bias=False)\n",
       "    )\n",
       "    (6): AttnProcessor2_0()\n",
       "    (7): IPAttnProcessor2_0(\n",
       "      (to_k_ip): Linear(in_features=768, out_features=640, bias=False)\n",
       "      (to_v_ip): Linear(in_features=768, out_features=640, bias=False)\n",
       "    )\n",
       "    (8): AttnProcessor2_0()\n",
       "    (9): IPAttnProcessor2_0(\n",
       "      (to_k_ip): Linear(in_features=768, out_features=1280, bias=False)\n",
       "      (to_v_ip): Linear(in_features=768, out_features=1280, bias=False)\n",
       "    )\n",
       "    (10): AttnProcessor2_0()\n",
       "    (11): IPAttnProcessor2_0(\n",
       "      (to_k_ip): Linear(in_features=768, out_features=1280, bias=False)\n",
       "      (to_v_ip): Linear(in_features=768, out_features=1280, bias=False)\n",
       "    )\n",
       "    (12): AttnProcessor2_0()\n",
       "    (13): IPAttnProcessor2_0(\n",
       "      (to_k_ip): Linear(in_features=768, out_features=1280, bias=False)\n",
       "      (to_v_ip): Linear(in_features=768, out_features=1280, bias=False)\n",
       "    )\n",
       "    (14): AttnProcessor2_0()\n",
       "    (15): IPAttnProcessor2_0(\n",
       "      (to_k_ip): Linear(in_features=768, out_features=1280, bias=False)\n",
       "      (to_v_ip): Linear(in_features=768, out_features=1280, bias=False)\n",
       "    )\n",
       "    (16): AttnProcessor2_0()\n",
       "    (17): IPAttnProcessor2_0(\n",
       "      (to_k_ip): Linear(in_features=768, out_features=1280, bias=False)\n",
       "      (to_v_ip): Linear(in_features=768, out_features=1280, bias=False)\n",
       "    )\n",
       "    (18): AttnProcessor2_0()\n",
       "    (19): IPAttnProcessor2_0(\n",
       "      (to_k_ip): Linear(in_features=768, out_features=640, bias=False)\n",
       "      (to_v_ip): Linear(in_features=768, out_features=640, bias=False)\n",
       "    )\n",
       "    (20): AttnProcessor2_0()\n",
       "    (21): IPAttnProcessor2_0(\n",
       "      (to_k_ip): Linear(in_features=768, out_features=640, bias=False)\n",
       "      (to_v_ip): Linear(in_features=768, out_features=640, bias=False)\n",
       "    )\n",
       "    (22): AttnProcessor2_0()\n",
       "    (23): IPAttnProcessor2_0(\n",
       "      (to_k_ip): Linear(in_features=768, out_features=640, bias=False)\n",
       "      (to_v_ip): Linear(in_features=768, out_features=640, bias=False)\n",
       "    )\n",
       "    (24): AttnProcessor2_0()\n",
       "    (25): IPAttnProcessor2_0(\n",
       "      (to_k_ip): Linear(in_features=768, out_features=320, bias=False)\n",
       "      (to_v_ip): Linear(in_features=768, out_features=320, bias=False)\n",
       "    )\n",
       "    (26): AttnProcessor2_0()\n",
       "    (27): IPAttnProcessor2_0(\n",
       "      (to_k_ip): Linear(in_features=768, out_features=320, bias=False)\n",
       "      (to_v_ip): Linear(in_features=768, out_features=320, bias=False)\n",
       "    )\n",
       "    (28): AttnProcessor2_0()\n",
       "    (29): IPAttnProcessor2_0(\n",
       "      (to_k_ip): Linear(in_features=768, out_features=320, bias=False)\n",
       "      (to_v_ip): Linear(in_features=768, out_features=320, bias=False)\n",
       "    )\n",
       "    (30): AttnProcessor2_0()\n",
       "    (31): IPAttnProcessor2_0(\n",
       "      (to_k_ip): Linear(in_features=768, out_features=1280, bias=False)\n",
       "      (to_v_ip): Linear(in_features=768, out_features=1280, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca356dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time steps: tensor([9.8096e-01, 9.6094e-01, 9.4092e-01, 9.2090e-01, 9.0088e-01, 8.8086e-01,\n",
      "        8.6084e-01, 8.4082e-01, 8.2129e-01, 8.0078e-01, 7.8125e-01, 7.6074e-01,\n",
      "        7.4121e-01, 7.2070e-01, 7.0117e-01, 6.8066e-01, 6.6113e-01, 6.4111e-01,\n",
      "        6.2109e-01, 6.0107e-01, 5.8105e-01, 5.6104e-01, 5.4102e-01, 5.2100e-01,\n",
      "        5.0098e-01, 4.8096e-01, 4.6094e-01, 4.4092e-01, 4.2090e-01, 4.0088e-01,\n",
      "        3.8086e-01, 3.6084e-01, 3.4082e-01, 3.2080e-01, 3.0078e-01, 2.8076e-01,\n",
      "        2.6123e-01, 2.4121e-01, 2.2119e-01, 2.0117e-01, 1.8115e-01, 1.6113e-01,\n",
      "        1.4111e-01, 1.2109e-01, 1.0107e-01, 8.1055e-02, 6.1035e-02, 4.1016e-02,\n",
      "        2.0996e-02, 9.7656e-04, 0.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "Samples shape: torch.Size([51, 2, 4, 64, 64]), dtype: torch.float16\n",
      "Samples shape: torch.Size([50, 4, 64, 64]), dtype: torch.float16\n",
      "Sampled images saved to ./outputs/tuning_few_steps/in1k256_sd15_klae_linear/demoimages/sampled_images.png\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPImageProcessor, CLIPVisionModelWithProjection\n",
    "\n",
    "\n",
    "image = Image.open(\"/home/ccy/code/IP-Adapter/assets/images/river.png\").convert(\"RGB\")\n",
    "clip_image_processor = CLIPImageProcessor()\n",
    "clip_image = clip_image_processor(images=image, return_tensors=\"pt\").pixel_values\n",
    "clip_image_embeds = image_encoder(clip_image.to(device, dtype=vae.dtype)).image_embeds\n",
    "# repeat 4 times\n",
    "num_samples = 2\n",
    "guidance_scale = 7.5\n",
    "sampling_steps = 50\n",
    "# clip_image_embeds = clip_image_embeds.repeat(4, 1, 1)\n",
    "\n",
    "# generate prompts\n",
    "num_prompts = clip_image_embeds.size(0)\n",
    "prompt = [\"best quality, high quality\"] * num_prompts\n",
    "negative_prompt = [\"monochrome, lowres, bad anatomy, worst quality, low quality\"] * num_prompts\n",
    "batch_size = len(prompt) if isinstance(prompt, list) else 1\n",
    "\n",
    "# encode image prompts\n",
    "image_prompt_embeds = model.image_proj_model(clip_image_embeds)\n",
    "uncond_image_prompt_embeds = model.image_proj_model(torch.zeros_like(clip_image_embeds))\n",
    "bs_embed, seq_len, _ = image_prompt_embeds.shape\n",
    "image_prompt_embeds = image_prompt_embeds.repeat(1, num_samples, 1).view(bs_embed * num_samples, seq_len, -1)\n",
    "uncond_image_prompt_embeds = uncond_image_prompt_embeds.repeat(1, num_samples, 1).view(bs_embed * num_samples, seq_len, -1)\n",
    "\n",
    "# encode text prompts\n",
    "text_input_ids = tokenizer(prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\").input_ids\n",
    "if text_input_ids.shape[-1] > tokenizer.model_max_length:\n",
    "    text_input_ids = text_input_ids[:, :tokenizer.model_max_length]\n",
    "uncond_input_ids = tokenizer(negative_prompt, padding=\"max_length\", max_length=text_input_ids.shape[-1], truncation=True, return_tensors=\"pt\").input_ids\n",
    "\n",
    "text_prompt_embeds = text_encoder(text_input_ids.to(device))[0]\n",
    "text_prompt_embeds = text_prompt_embeds.repeat(1, num_samples, 1).view(batch_size * num_samples, -1, text_prompt_embeds.shape[-1])\n",
    "text_negative_prompt_embeds = text_encoder(uncond_input_ids.to(device))[0]\n",
    "text_negative_prompt_embeds = text_negative_prompt_embeds.repeat(1, num_samples, 1).view(batch_size * num_samples, -1, text_negative_prompt_embeds.shape[-1])\n",
    "\n",
    "# concat prompt\n",
    "prompt_embeds = torch.cat([text_prompt_embeds, image_prompt_embeds], dim=1)\n",
    "negative_prompt_embeds = torch.cat([text_negative_prompt_embeds, uncond_image_prompt_embeds], dim=1)\n",
    "\n",
    "def cfg_model_wrapper(x_t, t, negative_prompt_embeds, prompt_embeds):\n",
    "    # Expand the latents for classifier-free guidance\n",
    "    if guidance_scale > 1.0:\n",
    "        latent_model_input = torch.cat([x_t] * 2)\n",
    "        encoder_hidden_states = torch.cat([negative_prompt_embeds, prompt_embeds])\n",
    "        t = t.repeat(2)  # Repeat t for both unconditioned and conditioned inputs\n",
    "    else:\n",
    "        latent_model_input = x_t\n",
    "        encoder_hidden_states = prompt_embeds\n",
    "    \n",
    "    # Predict noise\n",
    "    noise_pred = model.unet(\n",
    "        latent_model_input,\n",
    "        t * 1000,   # scale t to match the model's expected input\n",
    "        encoder_hidden_states=encoder_hidden_states,\n",
    "        return_dict=False,\n",
    "    )[0]\n",
    "    \n",
    "    # Apply classifier-free guidance\n",
    "    if guidance_scale > 1.0:\n",
    "        noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "        noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "    \n",
    "    return noise_pred\n",
    "\n",
    "# generator = get_generator(seed, device)\n",
    "with torch.inference_mode():\n",
    "    generator = get_generator(seed, device)\n",
    "    latent_channels = model.unet.config.in_channels\n",
    "    latents = torch.randn(\n",
    "        (num_prompts * num_samples, latent_channels, latent_size, latent_size),\n",
    "        device=device,\n",
    "        generator=generator,\n",
    "        dtype=vae.dtype\n",
    "    )\n",
    "\n",
    "    samples = unigen.sampling_loop(\n",
    "        inital_noise_z=latents,\n",
    "        sampling_model=cfg_model_wrapper,\n",
    "        sampling_steps=sampling_steps,\n",
    "        stochast_ratio=0.0,\n",
    "        extrapol_ratio=0.0,\n",
    "        sampling_order=1,\n",
    "        time_dist_ctrl=[1.0, 1.0, 1.0],\n",
    "        rfba_gap_steps=[0.019, 0.001],\n",
    "        prompt_embeds=prompt_embeds,\n",
    "        negative_prompt_embeds=negative_prompt_embeds,\n",
    "    )\n",
    "    print(f\"Samples shape: {samples.shape}, dtype: {samples.dtype}\")\n",
    "    samples = samples[1::2].reshape(-1, *samples.shape[2:])\n",
    "    print(f\"Samples shape: {samples.shape}, dtype: {samples.dtype}\")\n",
    "    with torch.no_grad():\n",
    "        # samples = (samples * dataset._latent_std.cuda()) / dataset.latent_multiplier + dataset._latent_mean.cuda()\n",
    "        samples = samples / vae.config.scaling_factor\n",
    "        samples = vae.decode(samples).sample\n",
    "        samples = torch.clamp((samples + 1) / 2, 0, 1)\n",
    "    save_image(samples, f\"{demoimages_dir}/sampled_images.png\", nrow=num_samples)\n",
    "    print(f\"Sampled images saved to {demoimages_dir}/sampled_images.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98bb36ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "torch.Size([50, 3, 512, 512])\n",
      "torch.float16\n",
      "0.18215\n"
     ]
    }
   ],
   "source": [
    "print(samples.device)\n",
    "print(samples.shape)\n",
    "print(latents.dtype)\n",
    "print(vae.config.scaling_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccaf4ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 512, 512, 3)\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=1024x512>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def image_grid(imgs, rows, cols):\n",
    "    assert len(imgs) == rows*cols\n",
    "\n",
    "    w, h = imgs[0].size\n",
    "    grid = Image.new('RGB', size=(cols*w, rows*h))\n",
    "    grid_w, grid_h = grid.size\n",
    "    \n",
    "    for i, img in enumerate(imgs):\n",
    "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
    "    return grid\n",
    "\n",
    "\n",
    "\n",
    "images = samples.cpu().permute(0, 2, 3, 1).numpy()\n",
    "print(images.shape)\n",
    "images = (images * 255).round().astype(\"uint8\")\n",
    "images = [Image.fromarray(image) for image in images]\n",
    "image_grid(images[-num_samples*num_prompts:], 1, 2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ucgm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
