{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a36078d9-c788-4323-b9af-88225e6c6c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import torch.backends.cuda\n",
    "import torch.distributed as dist\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "import os\n",
    "import random\n",
    "import argparse\n",
    "import json\n",
    "import time\n",
    "import itertools\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "from copy import deepcopy\n",
    "from accelerate import Accelerator\n",
    "from torchvision import transforms\n",
    "from transformers import CLIPImageProcessor, CLIPTokenizer, CLIPTextModel, CLIPVisionModelWithProjection\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel\n",
    "from PIL import Image\n",
    "\n",
    "# local imports\n",
    "from networks import NETWORKS\n",
    "from optimers import OPTIMERS\n",
    "from autoencs import AUTOENCS\n",
    "from methodes import METHODES\n",
    "from utilities import ImgLatentDataset\n",
    "from utilities import create_logger, load_config\n",
    "from utilities import set_seed, update_ema, remove_module_prefix, remove_module_all\n",
    "\n",
    "from ip_adapter.ip_adapter import ImageProjModel\n",
    "from ip_adapter.utils import is_torch2_available\n",
    "if is_torch2_available():\n",
    "    from ip_adapter.attention_processor import IPAttnProcessor2_0 as IPAttnProcessor, AttnProcessor2_0 as AttnProcessor\n",
    "else:\n",
    "    from ip_adapter.attention_processor import IPAttnProcessor, AttnProcessor\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, json_file, tokenizer, size=512, t_drop_rate=0.05, i_drop_rate=0.05, ti_drop_rate=0.05, image_root_path=\"\", vae=None, compute_latent_stats=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.size = size\n",
    "        self.i_drop_rate = i_drop_rate\n",
    "        self.t_drop_rate = t_drop_rate\n",
    "        self.ti_drop_rate = ti_drop_rate\n",
    "        self.image_root_path = image_root_path\n",
    "        self.vae = vae\n",
    "\n",
    "        self.data = json.load(open(json_file))\n",
    "\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(self.size, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "            transforms.CenterCrop(self.size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5], [0.5]),\n",
    "        ])\n",
    "        self.clip_image_processor = CLIPImageProcessor()\n",
    "        \n",
    "        # Initialize latent statistics\n",
    "        self._latent_mean = None\n",
    "        self._latent_std = None\n",
    "        self.latent_multiplier = 1.0\n",
    "        \n",
    "        # Compute latent statistics if requested\n",
    "        if compute_latent_stats and vae is not None:\n",
    "            self._compute_latent_statistics()\n",
    "        \n",
    "    def _compute_latent_statistics(self, sample_size=1000, batch_size=16):\n",
    "        \"\"\"\n",
    "        Compute mean and std of latent representations\n",
    "        \n",
    "        Args:\n",
    "            sample_size: Number of samples to use for statistics computation\n",
    "            batch_size: Batch size for processing\n",
    "        \"\"\"\n",
    "        print(\"Computing latent statistics...\")\n",
    "        \n",
    "        # Sample subset of data\n",
    "        sample_indices = np.random.choice(len(self.data), \n",
    "                                        min(sample_size, len(self.data)), \n",
    "                                        replace=False)\n",
    "        \n",
    "        latents = []\n",
    "        \n",
    "        # Temporarily disable dropout for consistent statistics\n",
    "        original_i_drop = self.i_drop_rate\n",
    "        original_t_drop = self.t_drop_rate\n",
    "        original_ti_drop = self.ti_drop_rate\n",
    "        self.i_drop_rate = 0.0\n",
    "        self.t_drop_rate = 0.0\n",
    "        self.ti_drop_rate = 0.0\n",
    "        \n",
    "        self.vae.eval()\n",
    "        with torch.no_grad():\n",
    "            for i in tqdm(range(0, len(sample_indices), batch_size), desc=\"Computing latent stats\"):\n",
    "                batch_indices = sample_indices[i:i+batch_size]\n",
    "                batch_images = []\n",
    "                \n",
    "                for idx in batch_indices:\n",
    "                    item = self.__getitem__(idx)\n",
    "                    batch_images.append(item[\"image\"])\n",
    "                \n",
    "                # Stack batch\n",
    "                batch_tensor = torch.stack(batch_images).to(self.vae.device, dtype=self.vae.dtype)\n",
    "                \n",
    "                posterior = self.vae.encode(batch_tensor).latent_dist\n",
    "                batch_latents = posterior.sample()  # or posterior.mean\n",
    "                \n",
    "                latents.append(batch_latents.cpu())\n",
    "        \n",
    "        # Restore original dropout rates\n",
    "        self.i_drop_rate = original_i_drop\n",
    "        self.t_drop_rate = original_t_drop\n",
    "        self.ti_drop_rate = original_ti_drop\n",
    "        \n",
    "        # Concatenate all latents\n",
    "        all_latents = torch.cat(latents, dim=0)\n",
    "        print(f\"Dimensions of all latents: {all_latents.shape}\")\n",
    "        \n",
    "        # Compute statistics\n",
    "        # Flatten spatial dimensions but keep channel dimension\n",
    "        # Shape: [N, C, H, W] -> [1, C, 1, 1]\n",
    "        self._latent_mean = all_latents.mean(dim=[0, 2, 3], keepdim=True)  # [1, C, 1, 1]\n",
    "        self._latent_std = all_latents.std(dim=[0, 2, 3], keepdim=True)  # [1, C, 1, 1]\n",
    "\n",
    "        # Prevent division by zero\n",
    "        self._latent_std = torch.clamp(self._latent_std, min=1e-6)\n",
    "        \n",
    "        # Set latent multiplier (can be tuned based on your needs)\n",
    "        self.latent_multiplier = 1.0 / self._latent_std.mean().item()\n",
    "        \n",
    "        print(f\"Latent mean shape: {self._latent_mean.shape}\")\n",
    "        print(f\"Latent std shape: {self._latent_std.shape}\")\n",
    "        print(f\"Latent multiplier: {self.latent_multiplier}\")\n",
    "        print(f\"Mean of latent mean: {self._latent_mean.mean().item():.6f}\")\n",
    "        print(f\"Mean of latent std: {self._latent_std.mean().item():.6f}\")\n",
    "\n",
    "    def get_latent_stats_cuda(self):\n",
    "        \"\"\"\n",
    "        Return latent statistics on CUDA\n",
    "        \"\"\"\n",
    "        if self._latent_mean is None or self._latent_std is None:\n",
    "            raise ValueError(\"Latent statistics not computed. Set compute_latent_stats=True during initialization.\")\n",
    "        \n",
    "        return (\n",
    "            self._latent_mean.cuda(),\n",
    "            self._latent_std.cuda(),\n",
    "            self.latent_multiplier\n",
    "        )\n",
    "    \n",
    "    def save_latent_stats(self, filepath):\n",
    "        \"\"\"\n",
    "        Save latent statistics to file\n",
    "        \"\"\"\n",
    "        if self._latent_mean is None or self._latent_std is None:\n",
    "            raise ValueError(\"No latent statistics to save.\")\n",
    "        \n",
    "        torch.save({\n",
    "            'latent_mean': self._latent_mean,\n",
    "            'latent_std': self._latent_std,\n",
    "            'latent_multiplier': self.latent_multiplier\n",
    "        }, filepath)\n",
    "        print(f\"Latent statistics saved to {filepath}\")\n",
    "    \n",
    "    def load_latent_stats(self, filepath):\n",
    "        \"\"\"\n",
    "        Load latent statistics from file\n",
    "        \"\"\"\n",
    "        stats = torch.load(filepath, map_location='cpu')\n",
    "        self._latent_mean = stats['latent_mean']\n",
    "        self._latent_std = stats['latent_std']\n",
    "        self.latent_multiplier = stats['latent_multiplier']\n",
    "        print(f\"Latent statistics loaded from {filepath}\")\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx] \n",
    "        text = item[\"text\"]\n",
    "        image_file = item[\"image_file\"]\n",
    "        \n",
    "        # read image\n",
    "        raw_image = Image.open(os.path.join(self.image_root_path, image_file))\n",
    "        image = self.transform(raw_image.convert(\"RGB\"))\n",
    "        clip_image = self.clip_image_processor(images=raw_image, return_tensors=\"pt\").pixel_values\n",
    "        \n",
    "        # drop\n",
    "        drop_image_embed = 0\n",
    "        rand_num = random.random()\n",
    "        if rand_num < self.i_drop_rate:\n",
    "            drop_image_embed = 1\n",
    "        elif rand_num < (self.i_drop_rate + self.t_drop_rate):\n",
    "            text = \"\"\n",
    "        elif rand_num < (self.i_drop_rate + self.t_drop_rate + self.ti_drop_rate):\n",
    "            text = \"\"\n",
    "            drop_image_embed = 1\n",
    "        # get text and tokenize\n",
    "        text_input_ids = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.tokenizer.model_max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ).input_ids\n",
    "        \n",
    "        return {\n",
    "            \"image\": image,\n",
    "            \"text_input_ids\": text_input_ids,\n",
    "            \"clip_image\": clip_image,\n",
    "            \"drop_image_embed\": drop_image_embed\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "\n",
    "def collate_fn(data):\n",
    "    images = torch.stack([example[\"image\"] for example in data])\n",
    "    text_input_ids = torch.cat([example[\"text_input_ids\"] for example in data], dim=0)\n",
    "    clip_images = torch.cat([example[\"clip_image\"] for example in data], dim=0)\n",
    "    drop_image_embeds = [example[\"drop_image_embed\"] for example in data]\n",
    "\n",
    "    return {\n",
    "        \"images\": images,\n",
    "        \"text_input_ids\": text_input_ids,\n",
    "        \"clip_images\": clip_images,\n",
    "        \"drop_image_embeds\": drop_image_embeds\n",
    "    }\n",
    "    \n",
    "\n",
    "class IPAdapter(torch.nn.Module):\n",
    "    \"\"\"IP-Adapter\"\"\"\n",
    "    def __init__(self, unet, image_proj_model, adapter_modules, ckpt_path=None):\n",
    "        super().__init__()\n",
    "        self.unet = unet\n",
    "        self.image_proj_model = image_proj_model\n",
    "        self.adapter_modules = adapter_modules\n",
    "\n",
    "        if ckpt_path is not None:\n",
    "            self.load_from_checkpoint(ckpt_path)\n",
    "    \n",
    "    def get_encoder_hidden_states(self, text_embeds, image_embeds):\n",
    "        \"\"\"\n",
    "        Get encoder hidden states with image embeddings projected by image_proj_model.\n",
    "        \"\"\"\n",
    "        ip_tokens = self.image_proj_model(image_embeds)\n",
    "        encoder_hidden_states = torch.cat([text_embeds, ip_tokens], dim=1)\n",
    "        return encoder_hidden_states\n",
    "    \n",
    "    def forward(self, noisy_latents, timesteps, encoder_hidden_states):\n",
    "        \"\"\"\n",
    "        Forward pass for the IP-Adapter.\n",
    "        Args:\n",
    "            noisy_latents: Noisy latents input to the UNet.\n",
    "            timesteps: Timesteps for the diffusion process.\n",
    "            encoder_hidden_states: Text embeddings concatenated with image embeddings projected by image_proj_model.\n",
    "        Returns:\n",
    "            noise_pred: Predicted noise residual from the UNet.\n",
    "        \"\"\"\n",
    "        # Predict the noise residual\n",
    "        noise_pred = self.unet(noisy_latents, timesteps, encoder_hidden_states).sample\n",
    "        return noise_pred\n",
    "\n",
    "    def load_from_checkpoint(self, ckpt_path: str):\n",
    "        # Calculate original checksums\n",
    "        orig_ip_proj_sum = torch.sum(torch.stack([torch.sum(p) for p in self.image_proj_model.parameters()]))\n",
    "        orig_adapter_sum = torch.sum(torch.stack([torch.sum(p) for p in self.adapter_modules.parameters()]))\n",
    "\n",
    "        state_dict = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "\n",
    "        # Load state dict for image_proj_model and adapter_modules\n",
    "        self.image_proj_model.load_state_dict(state_dict[\"image_proj\"], strict=True)\n",
    "        self.adapter_modules.load_state_dict(state_dict[\"ip_adapter\"], strict=True)\n",
    "\n",
    "        # Calculate new checksums\n",
    "        new_ip_proj_sum = torch.sum(torch.stack([torch.sum(p) for p in self.image_proj_model.parameters()]))\n",
    "        new_adapter_sum = torch.sum(torch.stack([torch.sum(p) for p in self.adapter_modules.parameters()]))\n",
    "\n",
    "        # Verify if the weights have changed\n",
    "        assert orig_ip_proj_sum != new_ip_proj_sum, \"Weights of image_proj_model did not change!\"\n",
    "        assert orig_adapter_sum != new_adapter_sum, \"Weights of adapter_modules did not change!\"\n",
    "\n",
    "        print(f\"Successfully loaded weights from checkpoint {ckpt_path}\")\n",
    "\n",
    "\n",
    "def decode_latents_to_images(vae, latents):\n",
    "    \"\"\"将AutoencoderKL的latents解码为图像\n",
    "    \n",
    "    Args:\n",
    "        vae: AutoencoderKL实例\n",
    "        latents: 潜在张量\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: [0, 1]范围的图像张量\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        decoded = vae.decode(latents).sample\n",
    "        images = torch.clamp((decoded + 1) / 2, 0, 1)\n",
    "        \n",
    "    return images\n",
    "\n",
    "\n",
    "def get_model(train_config, accelerator):\n",
    "    pretrained_model_name_or_path = train_config['model'][\"pretrained_model_name_or_path\"]\n",
    "    image_encoder_path = train_config['model'][\"image_encoder_path\"]\n",
    "    tokenizer = CLIPTokenizer.from_pretrained(pretrained_model_name_or_path, subfolder=\"tokenizer\")\n",
    "    text_encoder = CLIPTextModel.from_pretrained(pretrained_model_name_or_path, subfolder=\"text_encoder\")\n",
    "    vae = AutoencoderKL.from_pretrained(pretrained_model_name_or_path, subfolder=\"vae\")\n",
    "    unet = UNet2DConditionModel.from_pretrained(pretrained_model_name_or_path, subfolder=\"unet\")\n",
    "    image_encoder = CLIPVisionModelWithProjection.from_pretrained(image_encoder_path)\n",
    "    # freeze parameters of models to save more memory\n",
    "    unet.requires_grad_(False)\n",
    "    vae.requires_grad_(False)\n",
    "    text_encoder.requires_grad_(False)\n",
    "    image_encoder.requires_grad_(False)\n",
    "\n",
    "    # ip-adapter\n",
    "    image_proj_model = ImageProjModel(\n",
    "        cross_attention_dim=unet.config.cross_attention_dim,\n",
    "        clip_embeddings_dim=image_encoder.config.projection_dim,\n",
    "        clip_extra_context_tokens=4,\n",
    "    )\n",
    "    # init adapter modules\n",
    "    attn_procs = {}\n",
    "    unet_sd = unet.state_dict()\n",
    "    for name in unet.attn_processors.keys():\n",
    "        cross_attention_dim = None if name.endswith(\"attn1.processor\") else unet.config.cross_attention_dim\n",
    "        if name.startswith(\"mid_block\"):\n",
    "            hidden_size = unet.config.block_out_channels[-1]\n",
    "        elif name.startswith(\"up_blocks\"):\n",
    "            block_id = int(name[len(\"up_blocks.\")])\n",
    "            hidden_size = list(reversed(unet.config.block_out_channels))[block_id]\n",
    "        elif name.startswith(\"down_blocks\"):\n",
    "            block_id = int(name[len(\"down_blocks.\")])\n",
    "            hidden_size = unet.config.block_out_channels[block_id]\n",
    "        if cross_attention_dim is None:\n",
    "            attn_procs[name] = AttnProcessor()\n",
    "        else:\n",
    "            layer_name = name.split(\".processor\")[0]\n",
    "            weights = {\n",
    "                \"to_k_ip.weight\": unet_sd[layer_name + \".to_k.weight\"],\n",
    "                \"to_v_ip.weight\": unet_sd[layer_name + \".to_v.weight\"],\n",
    "            }\n",
    "            attn_procs[name] = IPAttnProcessor(hidden_size=hidden_size, cross_attention_dim=cross_attention_dim)\n",
    "            attn_procs[name].load_state_dict(weights)\n",
    "    unet.set_attn_processor(attn_procs)\n",
    "    adapter_modules = torch.nn.ModuleList(unet.attn_processors.values())\n",
    "    \n",
    "    ip_adapter = IPAdapter(unet, image_proj_model, adapter_modules, train_config['model'][\"pretrained_ip_adapter_path\"])\n",
    "    \n",
    "    weight_dtype = torch.float32\n",
    "    if accelerator.mixed_precision == \"fp16\":\n",
    "        weight_dtype = torch.float16\n",
    "    elif accelerator.mixed_precision == \"bf16\":\n",
    "        weight_dtype = torch.bfloat16\n",
    "    # unet.to(accelerator.device, dtype=weight_dtype)\n",
    "    vae.to(accelerator.device, dtype=weight_dtype)\n",
    "    text_encoder.to(accelerator.device, dtype=weight_dtype)\n",
    "    image_encoder.to(accelerator.device, dtype=weight_dtype)\n",
    "    # add\n",
    "    unet.to(accelerator.device, dtype=weight_dtype)\n",
    "    image_proj_model.to(accelerator.device, dtype=weight_dtype)\n",
    "    ip_adapter.to(accelerator.device, dtype=weight_dtype)\n",
    "\n",
    "\n",
    "    params_to_opt = itertools.chain(ip_adapter.image_proj_model.parameters(),  ip_adapter.adapter_modules.parameters())\n",
    "\n",
    "    return ip_adapter, params_to_opt, tokenizer, vae, text_encoder, image_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a71bc9-de68-4de4-b6c3-16c92fac3e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: Experiment directory created at ./outputs/tuning_few_steps/in1k256_sd15_klae_linear\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded weights from checkpoint /data/pretrained_models/IP-Adapter/models/ip-adapter_sd15.bin\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 23.65 GiB of which 3.81 MiB is free. Including non-PyTorch memory, this process has 23.64 GiB memory in use. Of the allocated memory 22.97 GiB is allocated by PyTorch, and 222.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 41\u001b[39m\n\u001b[32m     38\u001b[39m latent_size = train_config[\u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mimage_size\u001b[39m\u001b[33m\"\u001b[39m] // downsample_ratio\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# Load scheduler, tokenizer and models.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m model, params_to_opt, tokenizer, vae, text_encoder, image_encoder = \u001b[43mget_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccelerator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m model = model.to(device)\n\u001b[32m     43\u001b[39m params_to_opt_list = \u001b[38;5;28mlist\u001b[39m(params_to_opt)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 370\u001b[39m, in \u001b[36mget_model\u001b[39m\u001b[34m(train_config, accelerator)\u001b[39m\n\u001b[32m    368\u001b[39m image_encoder.to(accelerator.device, dtype=weight_dtype)\n\u001b[32m    369\u001b[39m \u001b[38;5;66;03m# add\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m370\u001b[39m \u001b[43munet\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43maccelerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight_dtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    371\u001b[39m image_proj_model.to(accelerator.device, dtype=weight_dtype)\n\u001b[32m    372\u001b[39m ip_adapter.to(accelerator.device, dtype=weight_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ucgm/lib/python3.11/site-packages/diffusers/models/modeling_utils.py:1353\u001b[39m, in \u001b[36mModelMixin.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1348\u001b[39m     logger.warning(\n\u001b[32m   1349\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe module \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m is group offloaded and moving it using `.to()` is not supported.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1350\u001b[39m     )\n\u001b[32m   1351\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1353\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ucgm/lib/python3.11/site-packages/torch/nn/modules/module.py:1340\u001b[39m, in \u001b[36mModule.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1337\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1338\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1340\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ucgm/lib/python3.11/site-packages/torch/nn/modules/module.py:900\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    898\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    899\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m900\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    902\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    903\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    904\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    905\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    910\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    911\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ucgm/lib/python3.11/site-packages/torch/nn/modules/module.py:900\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    898\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    899\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m900\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    902\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    903\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    904\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    905\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    910\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    911\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[31m[... skipping similar frames: Module._apply at line 900 (2 times)]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ucgm/lib/python3.11/site-packages/torch/nn/modules/module.py:900\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    898\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    899\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m900\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    902\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    903\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    904\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    905\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    910\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    911\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ucgm/lib/python3.11/site-packages/torch/nn/modules/module.py:927\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    923\u001b[39m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[32m    924\u001b[39m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[32m    925\u001b[39m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[32m    926\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m927\u001b[39m     param_applied = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    928\u001b[39m p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n\u001b[32m    930\u001b[39m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ucgm/lib/python3.11/site-packages/torch/nn/modules/module.py:1326\u001b[39m, in \u001b[36mModule.to.<locals>.convert\u001b[39m\u001b[34m(t)\u001b[39m\n\u001b[32m   1319\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t.dim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[32m4\u001b[39m, \u001b[32m5\u001b[39m):\n\u001b[32m   1320\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m t.to(\n\u001b[32m   1321\u001b[39m             device,\n\u001b[32m   1322\u001b[39m             dtype \u001b[38;5;28;01mif\u001b[39;00m t.is_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t.is_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1323\u001b[39m             non_blocking,\n\u001b[32m   1324\u001b[39m             memory_format=convert_to_format,\n\u001b[32m   1325\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1326\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1327\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1330\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1331\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1332\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) == \u001b[33m\"\u001b[39m\u001b[33mCannot copy out of meta tensor; no data!\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 23.65 GiB of which 3.81 MiB is free. Including non-PyTorch memory, this process has 23.64 GiB memory in use. Of the allocated memory 22.97 GiB is allocated by PyTorch, and 222.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# read config\n",
    "import os\n",
    "os.environ[\"NCCL_P2P_DISABLE\"] = \"1\"\n",
    "os.environ[\"NCCL_IB_DISABLE\"] = \"1\"\n",
    "\n",
    "args_config = \"./configs/tuning_few_steps/in1k256_sd15_klae_linear.yaml\"\n",
    "accelerator = Accelerator()\n",
    "train_config = load_config(args_config)\n",
    "par_path = args_config.split(\"/\")\n",
    "train_config[\"exp_name\"] = os.path.join(par_path[-2], par_path[-1].split(\".\")[0])\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Training a generative model.\n",
    "\"\"\"\n",
    "# Setup accelerator:\n",
    "device = accelerator.device\n",
    "rank = accelerator.process_index\n",
    "seed = train_config[\"train\"][\"global_seed\"] * accelerator.num_processes + rank\n",
    "set_seed(seed)\n",
    "\n",
    "experiment_dir = f\"{train_config['output_dir']}/{train_config['exp_name']}\"\n",
    "checkpoint_dir = f\"{experiment_dir}/checkpoints\"\n",
    "\n",
    "# Setup an experiment folder:\n",
    "if accelerator.is_main_process:\n",
    "    os.makedirs(train_config[\"output_dir\"], exist_ok=True)\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    logger = create_logger(experiment_dir, \"train\")\n",
    "    logger.info(f\"Experiment directory created at {experiment_dir}\")\n",
    "\n",
    "# Create model:\n",
    "downsample_ratio = train_config[\"vae\"][\"downsample_ratio\"]\n",
    "assert (\n",
    "    train_config[\"data\"][\"image_size\"] % downsample_ratio == 0\n",
    "), \"Image size must be divisible by 8 (for the VAE encoder).\"\n",
    "latent_size = train_config[\"data\"][\"image_size\"] // downsample_ratio\n",
    "# Load scheduler, tokenizer and models.\n",
    "\n",
    "model, params_to_opt, tokenizer, vae, text_encoder, image_encoder = get_model(train_config, accelerator)\n",
    "model = model.to(device)\n",
    "params_to_opt_list = list(params_to_opt)\n",
    "print(f\"[{rank}] {len(params_to_opt_list)}, params to optimize\")\n",
    "\n",
    "ema = deepcopy(model).requires_grad_(False).to(device)\n",
    "\n",
    "if accelerator.is_main_process:\n",
    "    demoimages_dir = f\"{experiment_dir}/demoimages\"\n",
    "    os.makedirs(demoimages_dir, exist_ok=True)\n",
    "\n",
    "    # 准备demo数据\n",
    "    # demo_texts = [\"a beautiful landscape\", \"a cute cat\"]\n",
    "    # demo_text_inputs = tokenizer(\n",
    "    #     demo_texts,\n",
    "    #     max_length=tokenizer.model_max_length,\n",
    "    #     padding=\"max_length\",\n",
    "    #     truncation=True,\n",
    "    #     return_tensors=\"pt\"\n",
    "    # ).input_ids.to(device)\n",
    "    \n",
    "    # 创建假的图像embeddings或使用真实图像\n",
    "    # batch_size = len(demo_texts)\n",
    "    # demo_image_embeds = torch.randn(\n",
    "    #     batch_size, \n",
    "    #     image_encoder.config.projection_dim, \n",
    "    #     device=device,\n",
    "    #     dtype=image_encoder.dtype\n",
    "    # )\n",
    "    # demo_image_embeds = torch.zeros(\n",
    "    #     batch_size, \n",
    "    #     image_encoder.config.projection_dim, \n",
    "    #     device=device,\n",
    "    #     dtype=image_encoder.dtype\n",
    "    # )\n",
    "    \n",
    "    # 生成text embeddings和最终的encoder hidden states\n",
    "    # with torch.no_grad():\n",
    "    #     demo_text_embeds = text_encoder(demo_text_inputs)[0]\n",
    "    #     demo_y = ema.get_encoder_hidden_states(demo_text_embeds, demo_image_embeds)\n",
    "    \n",
    "    # latent_channels = vae.config.latent_channels\n",
    "    # demo_z = torch.randn(\n",
    "    #     batch_size, latent_channels, latent_size, latent_size, device=device\n",
    "    # )\n",
    "    # print(f\"Demo z shape: {demo_z.shape}, demo_y shape: {demo_y.shape}\")\n",
    "\n",
    "unigen = METHODES[\"unigen\"](\n",
    "    transport_type=train_config[\"transport\"][\"type\"],\n",
    "    lab_drop_ratio=train_config[\"transport\"][\"lab_drop_ratio\"],\n",
    "    consistc_ratio=train_config[\"transport\"][\"consistc_ratio\"],\n",
    "    enhanced_ratio=train_config[\"transport\"][\"enhanced_ratio\"],\n",
    "    enhanced_style=train_config[\"transport\"][\"enhanced_style\"],\n",
    "    scaled_cbl_eps=train_config[\"transport\"][\"scaled_cbl_eps\"],\n",
    "    ema_decay_rate=train_config[\"transport\"][\"ema_decay_rate\"],\n",
    "    enhanced_range=train_config[\"transport\"][\"enhanced_range\"],\n",
    "    time_dist_ctrl=train_config[\"transport\"][\"time_dist_ctrl\"],\n",
    "    wt_cosine_loss=train_config[\"transport\"][\"wt_cosine_loss\"],\n",
    "    weight_funcion=train_config[\"transport\"][\"weight_funcion\"],\n",
    ")\n",
    "if accelerator.is_main_process:\n",
    "    logger.info(\n",
    "        f\"SD15 parameters: {sum(p.numel() for p in params_to_opt_list) / 1e6:.2f}M\"\n",
    "    )\n",
    "    logger.info(\n",
    "        f\"Optimizer: {train_config['optimizer']['type']}, lr={train_config['optimizer']['lr']}, beta1={train_config['optimizer']['beta1']}, beta2={train_config['optimizer']['beta2']}\"\n",
    "    )\n",
    "    logger.info(f'Use cosine loss: {train_config[\"transport\"][\"wt_cosine_loss\"]}')\n",
    "    logger.info(f'Use weight func: {train_config[\"transport\"][\"weight_funcion\"]}')\n",
    "\n",
    "opt = OPTIMERS[train_config[\"optimizer\"][\"type\"]](\n",
    "    params_to_opt_list,\n",
    "    lr=train_config[\"optimizer\"][\"lr\"],\n",
    "    weight_decay=train_config[\"optimizer\"][\"weight_decay\"],\n",
    "    betas=(train_config[\"optimizer\"][\"beta1\"], train_config[\"optimizer\"][\"beta2\"]),\n",
    ")\n",
    "\n",
    "# tokenizer = CLIPTokenizer.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"tokenizer\")\n",
    "dataset = MyDataset(\n",
    "    json_file=train_config[\"data\"][\"json_file\"],\n",
    "    tokenizer=tokenizer,\n",
    "    size=train_config[\"data\"][\"image_size\"],\n",
    "    image_root_path=train_config[\"data\"][\"image_root_path\"],\n",
    "    vae=vae,\n",
    "    compute_latent_stats=True,\n",
    ")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "batch_size_per_gpu = (\n",
    "    train_config[\"train\"][\"global_batch_size\"] // accelerator.num_processes\n",
    ")\n",
    "\n",
    "global_batch_size = batch_size_per_gpu * accelerator.num_processes\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size_per_gpu,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=train_config[\"data\"][\"num_workers\"],\n",
    "    pin_memory=True,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "if accelerator.is_main_process:\n",
    "    mean, stad, latent_multiplier = (\n",
    "        dataset._latent_mean.cuda(),\n",
    "        dataset._latent_std.cuda(),\n",
    "        dataset.latent_multiplier,\n",
    "    )\n",
    "    # logger.info(\n",
    "    #     f\"Dataset contains {len(dataset):,} images {train_config['data']['data_path']}\"\n",
    "    # )\n",
    "    logger.info(\n",
    "        f\"Batch size {batch_size_per_gpu} per gpu, with {global_batch_size} global batch size\"\n",
    "    )\n",
    "\n",
    "if \"ckpt\" in train_config[\"train\"]:\n",
    "    checkpoint_path = f\"{checkpoint_dir}/{train_config['train']['ckpt']}\"\n",
    "    checkpoint = torch.load(\n",
    "        checkpoint_path, map_location=lambda storage, loc: storage\n",
    "    )\n",
    "    model.load_state_dict(checkpoint[\"model\"])\n",
    "    if train_config[\"train\"][\"no_reopt\"] is not True:\n",
    "        opt.load_state_dict(checkpoint[\"opt\"])\n",
    "    if train_config[\"train\"][\"no_reuni\"] is not True:\n",
    "        if ((unigen.cor > 0.0) or (unigen.enr > 0.0)) and unigen.emd > 0.0:\n",
    "            unigen.mod = deepcopy(model).requires_grad_(False).to(device)\n",
    "        unigen.load_state_dict(checkpoint[\"unigen\"])\n",
    "    ema.load_state_dict(checkpoint[\"ema\"])\n",
    "    train_steps = int(checkpoint_path.split(\"/\")[-1].split(\".\")[0])\n",
    "    del checkpoint\n",
    "    if accelerator.is_main_process:\n",
    "        logger.info(f\"Loaded checkpoint at: {checkpoint_path}.\")\n",
    "else:\n",
    "    train_steps = 0\n",
    "    update_ema(ema, model, decay=0)\n",
    "    if accelerator.is_main_process:\n",
    "        logger.info(\"Starting training from scratch.\")\n",
    "\n",
    "# Prepare models for training:\n",
    "model.train()\n",
    "ema.eval()\n",
    "if train_config[\"train\"][\"no_buffer\"] is True:\n",
    "    model = DDP(model, device_ids=[rank], broadcast_buffers=False)\n",
    "model, opt, loader, unigen = accelerator.prepare(model, opt, loader, unigen)\n",
    "\n",
    "# Variables for monitoring/logging purposes:\n",
    "log_steps = 0\n",
    "running_loss = 0\n",
    "start_time = time()\n",
    "\n",
    "while True:\n",
    "    for batch in loader:\n",
    "        # Convert images to latent space\n",
    "        with torch.no_grad():\n",
    "            latents = vae.encode(batch[\"images\"].to(accelerator.device, dtype=vae.dtype)).latent_dist.sample()\n",
    "            latents = latents * vae.config.scaling_factor\n",
    "\n",
    "            image_embeds = image_encoder(batch[\"clip_images\"].to(accelerator.device, dtype=image_encoder.dtype)).image_embeds\n",
    "            text_embeds = text_encoder(batch[\"text_input_ids\"].to(accelerator.device))[0]\n",
    "\n",
    "        image_embeds_ = []\n",
    "        for image_embed, drop_image_embed in zip(image_embeds, batch[\"drop_image_embeds\"]):\n",
    "            if drop_image_embed == 1:\n",
    "                image_embeds_.append(torch.zeros_like(image_embed))\n",
    "            else:\n",
    "                image_embeds_.append(image_embed)\n",
    "        image_embeds = torch.stack(image_embeds_)\n",
    "\n",
    "        x = latents\n",
    "        y = model.module.get_encoder_hidden_states(text_embeds, image_embeds)\n",
    "\n",
    "        loss = unigen.training_step(model, x, y)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        accelerator.backward(loss)\n",
    "        if \"max_grad_norm\" in train_config[\"optimizer\"]:\n",
    "            if accelerator.sync_gradients:\n",
    "                accelerator.clip_grad_norm_(\n",
    "                    model.parameters(), train_config[\"optimizer\"][\"max_grad_norm\"]\n",
    "                )\n",
    "        for param in model.parameters():\n",
    "            if param.grad is not None:\n",
    "                torch.nan_to_num_(param.grad, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        opt.step()\n",
    "        update_ema(ema, model, train_config[\"train\"][\"ema_decay\"])\n",
    "\n",
    "        # Log loss values:\n",
    "        running_loss += loss.item()\n",
    "        log_steps += 1\n",
    "        train_steps += 1\n",
    "        if train_steps % train_config[\"train\"][\"log_every\"] == 0:\n",
    "            # Measure training speed:\n",
    "            torch.cuda.synchronize()\n",
    "            end_time = time()\n",
    "            steps_per_sec = log_steps / (end_time - start_time)\n",
    "            # Reduce loss history over all processes:\n",
    "            avg_loss = torch.tensor(running_loss / log_steps, device=device)\n",
    "            dist.all_reduce(avg_loss, op=dist.ReduceOp.SUM)\n",
    "            avg_loss = avg_loss.item() / dist.get_world_size()\n",
    "            if accelerator.is_main_process:\n",
    "                logger.info(\n",
    "                    f\"(step={train_steps:07d}) Train Loss: {avg_loss:.4f}, Train Steps/Sec: {steps_per_sec:.2f}\"\n",
    "                )\n",
    "            # Reset monitoring variables:\n",
    "            running_loss = 0\n",
    "            log_steps = 0\n",
    "            start_time = time()\n",
    "\n",
    "        # Save checkpoint:\n",
    "        if (\n",
    "            train_steps % train_config[\"train\"][\"ckpt_every\"] == 0\n",
    "            and train_steps > 0\n",
    "        ):\n",
    "            if accelerator.is_main_process:\n",
    "                checkpoint = {\n",
    "                    \"model\": remove_module_prefix(model.state_dict()),\n",
    "                    \"ema\": ema.state_dict(),\n",
    "                    \"opt\": opt.state_dict(),\n",
    "                    \"unigen\": remove_module_all(unigen.state_dict()),\n",
    "                    \"config\": train_config,\n",
    "                }\n",
    "                checkpoint_path = f\"{checkpoint_dir}/{train_steps:07d}.pt\"\n",
    "                torch.save(checkpoint, checkpoint_path)\n",
    "                logger.info(f\"Saved checkpoint to {checkpoint_path}\")\n",
    "\n",
    "                # demox = unigen.sampling_loop(demo_z, ema, **dict(encoder_hidden_states=demo_y))\n",
    "                # demox = demox[1::2].reshape(-1, *demox.shape[2:])\n",
    "                # print(f\"Demox images shape: {demox.shape}, stad shape: {stad.shape}, mean shape: {mean.shape}\")\n",
    "                # demox = (demox * stad) / latent_multiplier + mean\n",
    "                # demox = decode_latents_to_images(vae, demox).cpu()\n",
    "                # demoimages_path = f\"{demoimages_dir}/{train_steps:07d}.png\"\n",
    "                # save_image(demox, os.path.join(demoimages_path), nrow=len(demo_y))\n",
    "                # logger.info(f\"Saved demoimages to {demoimages_path}\")\n",
    "                # del checkpoint, demox\n",
    "\n",
    "            dist.barrier()\n",
    "\n",
    "        if train_steps >= train_config[\"train\"][\"max_steps\"]:\n",
    "            break\n",
    "    if train_steps >= train_config[\"train\"][\"max_steps\"]:\n",
    "        break\n",
    "if accelerator.is_main_process:\n",
    "    logger.info(\"Done!\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ucgm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
